#!/usr/bin/env python3
"""
Google Jobs Background Scheduler
Runs independently to continuously populate Supabase with fresh Google Jobs
No user engagement required - just runs in background and stores quality jobs
"""

import time
import schedule
import logging
from datetime import datetime
from google_jobs_storage import GoogleJobsStorage

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('google_jobs_scheduler.log'),
        logging.StreamHandler()
    ]
)

class GoogleJobsScheduler:
    def __init__(self):
        self.storage = GoogleJobsStorage()
        self.target_locations = [
            "Houston, TX",
            "Dallas, TX", 
            "Austin, TX",
            "San Antonio, TX",
            "Phoenix, AZ",
            "Las Vegas, NV",
            "Denver, CO",
            "Newark, NJ"
        ]
        
    def run_location_harvest(self, location):
        """Harvest Google Jobs for a single location"""
        try:
            logging.info(f"ğŸ” Starting Google Jobs harvest for {location}")
            
            stored_count = self.storage.scrape_and_store_google_jobs(
                location=location,
                search_terms="CDL Driver",
                limit=100  # Get reasonable number of jobs
            )
            
            logging.info(f"âœ… {location}: Stored {stored_count} quality jobs")
            return stored_count
            
        except Exception as e:
            logging.error(f"âŒ {location} harvest failed: {e}")
            return 0
    
    def run_full_harvest_cycle(self):
        """Run full harvest cycle across all target locations"""
        logging.info("ğŸš€ STARTING FULL GOOGLE JOBS HARVEST CYCLE")
        logging.info("=" * 60)
        
        total_stored = 0
        successful_locations = 0
        
        for location in self.target_locations:
            stored = self.run_location_harvest(location)
            total_stored += stored
            
            if stored > 0:
                successful_locations += 1
                
            # Add delay between locations to be respectful to API
            time.sleep(10)
        
        logging.info("=" * 60)
        logging.info(f"ğŸ‰ HARVEST CYCLE COMPLETE")
        logging.info(f"ğŸ“Š Results: {successful_locations}/{len(self.target_locations)} locations successful")
        logging.info(f"ğŸ’¾ Total jobs stored: {total_stored}")
        logging.info(f"â° Next cycle: {datetime.now().strftime('%H:%M:%S')}")
        
        return total_stored

def main():
    """Run the background scheduler"""
    scheduler = GoogleJobsScheduler()
    
    logging.info("ğŸ¤– Google Jobs Background Scheduler Starting")
    logging.info("ğŸ“… Schedule: Every 6 hours")
    logging.info("ğŸ“ Target locations: 8 markets")
    logging.info("ğŸ”„ Running continuously...")
    
    # Schedule job every 6 hours
    schedule.every(6).hours.do(scheduler.run_full_harvest_cycle)
    
    # Run once immediately on startup
    logging.info("ğŸš€ Running initial harvest cycle...")
    scheduler.run_full_harvest_cycle()
    
    # Keep running
    while True:
        schedule.run_pending()
        time.sleep(60)  # Check every minute

if __name__ == "__main__":
    main()